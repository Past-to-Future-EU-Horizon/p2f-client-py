{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76d408e",
   "metadata": {},
   "source": [
    "# P2F Python Client Library Example\n",
    "\n",
    "Welcome to the Client Library Example Notebook.\n",
    "\n",
    "First, we need to import a few things, the P2F Client library and the p2f_pydantic library. The client library is an implementation of requests and will send and recieve pydantic objects between your computer and the API. \n",
    "\n",
    "## Pydantic?\n",
    "\n",
    "Pydantic is a library that works on the back-end of the API to enforce data types and converting data structures to JSON.  \n",
    "\n",
    "So why do you need it? So we can have the clients and the server in sync and agree on the definition of data. The library will also encode your local data into JSON for requests to send over the REST API standard in a fairly painless manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4aa963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from p2f_client.p2f_client import P2F_Client\n",
    "import p2f_pydantic\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906ac56",
   "metadata": {},
   "source": [
    "First, let's initialize a client, this method will probably get updated in the future, but below this is the initialization method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = P2F_Client(hostname=\"localhost\", port=8000, https=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2afa2c",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Below we're going to start by uploading new datasets to the Portal. What is happening is I am creating a pydantic Datasets object for each of my three datasets. \n",
    "\n",
    "After that, there are two methods of uploading, what is shown here is `add_dataset`, which places the dataset into a queue that can then all be uploaded at once with `upload_datasets`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pydantic Objects\n",
    "dataset1 = p2f_pydantic.datasets.Datasets(doi=\"10.1594/PANGAEA.920596\",\n",
    "                                          title=\"Last Glacial Maximum SST proxy collection and data assimilation\",\n",
    "                                          publication_date=datetime.datetime(2020, 7, 21),\n",
    "                                          is_new_p2f=False,\n",
    "                                          is_sub_dataset=False)\n",
    "\n",
    "dataset2 = p2f_pydantic.datasets.Datasets(doi=\"10.5194/gmd-12-3149-2019-supplement\",\n",
    "                                          title=\"The DeepMIP contribution to PMIP4: methodologies for selection, compilation and analysis of latest Paleocene and early Eocene climate proxy data, incorporating version 0.1 of the DeepMIP database\",\n",
    "                                          publication_date=datetime.datetime(2019, 7, 25),\n",
    "                                          is_new_p2f=False,\n",
    "                                          is_sub_dataset=False)\n",
    "\n",
    "dataset3 = p2f_pydantic.datasets.Datasets(doi=\"10.1594/PANGAEA.911847\", \n",
    "                                          title=\"Sea surface temperature anomalies for Pliocene interglacial KM5c (PlioVAR)\", \n",
    "                                          publication_date=datetime.datetime(2020, 2, 7), \n",
    "                                          is_new_p2f=False, \n",
    "                                          is_sub_dataset=False)\n",
    "\n",
    "# Add datasets to Queue\n",
    "client.datasets.add_dataset(dataset1)\n",
    "client.datasets.add_dataset(dataset2)\n",
    "client.datasets.add_dataset(dataset3)\n",
    "\n",
    "# Upload datasets in the Queue\n",
    "client.datasets.upload_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d2c95",
   "metadata": {},
   "source": [
    "You can also just upload a dataset directly with `upload_dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4 = p2f_pydantic.datasets.Datasets(doi=\"10.5194/gmd-12-3149-2019-supplement\", \n",
    "                                          title=\"The DeepMIP contribution to PMIP4: methodologies for selection, compilation and analysis of latest Paleocene and early Eocene climate proxy data, incorporating version 0.1 of the DeepMIP database\",\n",
    "                                          sub_dataset_namee=\"SDF02_Sites.xlsx\",\n",
    "                                          publication_date=datetime.datetime(2019, 7, 25), \n",
    "                                          is_new_p2f=False, \n",
    "                                          is_sub_dataset=True)\n",
    "client.datasets.upload_dataset(dataset4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a705d8",
   "metadata": {},
   "source": [
    "Now that we have datasets uploaded, we can check datasets that exist on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe294e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of datasets on the server\n",
    "client.datasets.list_remote_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c83c08",
   "metadata": {},
   "source": [
    "You can also delete a dataset, below I am just deleting the last dataset on the server. **Please don't do this in real usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.datasets.delete_remote_dataset(client.datasets.list_remote_datasets()[-1].dataset_identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e2f31",
   "metadata": {},
   "source": [
    "## Adding a Table\n",
    "\n",
    "Adding a table is going to be a bit more complex, we need to prepopulate our data types, and then iterate through our table. \n",
    "\n",
    "For this example we're going to look at one of the tables (sub-datasets) of the PlioVAR article from above. First let's add our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b72545",
   "metadata": {},
   "outputs": [],
   "source": [
    "pliovartab = p2f_pydantic.datasets.Datasets(doi=\"10.1594/PANGAEA.911847\", \n",
    "                                            title=\"Sea surface temperature anomalies for Pliocene interglacial KM5c (PlioVAR)\", \n",
    "                                            sub_dataset_name=\"PlioVAR-KM5c_T.tab\", \n",
    "                                            publication_date=datetime.datetime(2020, 2, 7), \n",
    "                                            is_new_p2f=False, \n",
    "                                            is_sub_dataset=True)\n",
    "pliovartab = client.datasets.upload_dataset(pliovartab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4dc69",
   "metadata": {},
   "source": [
    "For this dataset we have 20 columns, four of which can be grouped into being a location, and nine that are Sea Surface Temperature related. Let's check the API server for what data types exist for Sea Surface Temperatures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96214749",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.harm_data_type.list_data_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98b835",
   "metadata": {},
   "source": [
    "For me, the developer, there are currently no data types on the P2F API server, so I need to create them. \n",
    "\n",
    "Reading the documentation on the dataset I know I need to create a data type for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base SSTs\n",
    "SST_UK37 = p2f_pydantic.harm_data_types.harm_data_type(measure=\"Sea Surface Temperature\", \n",
    "                                                       unit_of_measurement=\"°C\",\n",
    "                                                       method=\"UK37\",\n",
    "                                                       is_proxy=True)\n",
    "SST_UK37_BAYSPLINE = p2f_pydantic.harm_data_types.harm_data_type(measure=\"Sea Surface Temperature\", \n",
    "                                                       unit_of_measurement=\"°C\",\n",
    "                                                       method=\"UK37 with Bayspline calibration\",\n",
    "                                                       is_proxy=True)\n",
    "SST_MgCa = p2f_pydantic.harm_data_types.harm_data_type(measure=\"Sea Surface Temperature\", \n",
    "                                                       unit_of_measurement=\"°C\",\n",
    "                                                       method=\"Ratios Mg/Ca\",\n",
    "                                                       is_proxy=True)\n",
    "SST_MgCa_BAYMAG = p2f_pydantic.harm_data_types.harm_data_type(measure=\"Sea Surface Temperature\", \n",
    "                                                       unit_of_measurement=\"°C\",\n",
    "                                                       method=\"Ratios Mg/Ca with BAYMAG calibration\",\n",
    "                                                       is_proxy=True)\n",
    "SST_FORWARD = p2f_pydantic.harm_data_types.harm_data_type(measure=\"Sea Surface Temperature\", \n",
    "                                                       unit_of_measurement=\"°C\",\n",
    "                                                       method=\"Forward-modelled 'core-top' Mg/Ca sea surface temperature from BAYMAG\",\n",
    "                                                       is_proxy=True)\n",
    "\n",
    "# Anomalous SSTs\n",
    "SSTA_UK37 = p2f_pydantic.harm_data_types.harm_data_type(measure=\"Sea Surface Temperature Anomaly\", \n",
    "                                                       unit_of_measurement=\"°C\",\n",
    "                                                       method=\"UK37\",\n",
    "                                                       is_proxy=True)\n",
    "SSTA_UK37_BAYSPLINE = p2f_pydantic.harm_data_types.harm_data_type(measure=\"Sea Surface Temperature Anomaly\", \n",
    "                                                       unit_of_measurement=\"°C\",\n",
    "                                                       method=\"UK37 with Bayspline calibration\",\n",
    "                                                       is_proxy=True)\n",
    "SSTA_MgCa = p2f_pydantic.harm_data_types.harm_data_type(measure=\"Sea Surface Temperature Anomaly\", \n",
    "                                                       unit_of_measurement=\"°C\",\n",
    "                                                       method=\"Ratios Mg/Ca\",\n",
    "                                                       is_proxy=True)\n",
    "SSTA_MgCa_BAYMAG = p2f_pydantic.harm_data_types.harm_data_type(measure=\"Sea Surface Temperature Anomaly\", \n",
    "                                                       unit_of_measurement=\"°C\",\n",
    "                                                       method=\"Ratios Mg/Ca with BAYMAG calibration\",\n",
    "                                                       is_proxy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32136bd7",
   "metadata": {},
   "source": [
    "After we create the data types, we upload them individually to the API so we can get their `datatype_id` back for use with our numerical uploads later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b851e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_UK37 = client.harm_data_type.upload_data_type(SST_UK37)\n",
    "SSTA_UK37 = client.harm_data_type.upload_data_type(SSTA_UK37)\n",
    "SST_UK37_BAYSPLINE = client.harm_data_type.upload_data_type(SST_UK37_BAYSPLINE)\n",
    "SSTA_UK37_BAYSPLINE = client.harm_data_type.upload_data_type(SSTA_UK37_BAYSPLINE)\n",
    "SST_MgCa = client.harm_data_type.upload_data_type(SST_MgCa)\n",
    "SSTA_MgCa = client.harm_data_type.upload_data_type(SSTA_MgCa)\n",
    "SST_MgCa_BAYMAG = client.harm_data_type.upload_data_type(SST_MgCa_BAYMAG)\n",
    "SSTA_MgCa_BAYMAG = client.harm_data_type.upload_data_type(SSTA_MgCa_BAYMAG)\n",
    "SST_FORWARD = client.harm_data_type.upload_data_type(SST_FORWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207563d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm upload\n",
    "client.harm_data_type.list_data_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f76df5",
   "metadata": {},
   "source": [
    "Next let's load our dataset into Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plio_df = pd.read_csv(\"Pliocene_SSTs/PlioVAR-KM5c_T.tab\", skiprows=99, sep=\"\\t\")\n",
    "plio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180c3b9",
   "metadata": {},
   "source": [
    "Here we will iterate through all of the records using `iterrows()` and then for each row of the table we will add in some information. \n",
    "\n",
    "**Note on row hash** - I was trying to create a universal and unique (kind of a UUID) way for a row to be referred to as without just using the row number but also to be recalculable over and over again. This may be a pointless thing to do here, but it is currently the way its implemented. \n",
    "\n",
    "The steps as seen below are:\n",
    "\n",
    "1. Calculate a row hash, and insert the row as a data record that will continue to be referred to by its record_hash.\n",
    "2. Create (or find an existing) location record and add to the database, then assign location to the data record.\n",
    "3. Then for each data type, add the numerical records as long as they exist, if they are null, in my opinion it is best not to add in empty data (this can be a point for reconsideration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6eee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the UUID of the dataset after it was created by the database above\n",
    "dataset_id = pliovartab.dataset_identifier\n",
    "\n",
    "# Iterate through the rows using iterrows, ix is index and row is the rest of the content of the row\n",
    "for ix, row in plio_df.iterrows():\n",
    "    # Calculate the row hash\n",
    "    row_hash = client.harm_data_records.calculate_hash(dataset_id=dataset_id,\n",
    "                                                       row_number=ix,\n",
    "                                                       debugging=True)\n",
    "    # Create the data record object for uploading\n",
    "    row_record = p2f_pydantic.harm_data_record.harm_data_record(fk_dataset=dataset_id,\n",
    "                                                                record_hash=row_hash)\n",
    "    # Upload the data record\n",
    "    client.harm_data_records.upload_data_record(row_record)\n",
    "    # Create the location object for uploading\n",
    "    location = p2f_pydantic.harm_data_metadata.harm_location(location_name=row.iloc[1],\n",
    "                                                             latitude=row.iloc[2],\n",
    "                                                             longitude=row.iloc[3],\n",
    "                                                             elevation=row.iloc[4], \n",
    "                                                             location_age=0)\n",
    "    # Upload the location to the database\n",
    "    location = client.harm_location.upload_harm_location(location) \n",
    "    # Assign that newly created location to this data record\n",
    "    client.harm_location.assign_location_to_record(location.location_identifier, row_hash)\n",
    "\n",
    "    # Create some lists that we can use indexes on to reuse code\n",
    "    #     This first one has the data types from above so we can grab\n",
    "    #      their datatype_id\n",
    "    col_datatype = [SST_UK37, SST_UK37_BAYSPLINE, SST_MgCa, \n",
    "                    SST_MgCa_BAYMAG, SST_FORWARD, SSTA_UK37,\n",
    "                    SSTA_UK37_BAYSPLINE, SSTA_MgCa, SSTA_MgCa_BAYMAG] \n",
    "    #     This one is the column number and this order matches the order\n",
    "    #      of the data in the above list. \n",
    "    cols = [5, 6, 7, 8, 10, 12, 13, 14, 15]\n",
    "    # Iterate through the column numbers\n",
    "    for col in cols:\n",
    "        # Check that the value is not null\n",
    "        if not np.isnan(row.iloc[col]):\n",
    "            # Create the numerical insert object for uploading\n",
    "            new_numerical_insert = p2f_pydantic.harm_data_numerical.insert_harm_numerical(fk_data_record=row_hash,\n",
    "                                                                                          fk_data_type=col_datatype[cols.index(col)].datatype_id,\n",
    "                                                                                          numerical_type=\"FLOAT\",\n",
    "                                                                                          value=row.iloc[col])\n",
    "            # Upload the numerical object\n",
    "            client.harm_numerical.upload_harm_numerical(new_numerical_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b6c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2f-client-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
